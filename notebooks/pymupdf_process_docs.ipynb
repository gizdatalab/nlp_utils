{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from nlputils import utils\n",
    "from nlputils.components.pymupdf_util import pymuprocessor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Declare the variables #######\n",
    "\n",
    "path_to_files_list = \"path to files from list from **load_docs.ipynb**\"\n",
    "\n",
    "path_to_save_output_from_pymuprocessor = \"path where the output from axaparsr needs to save the files\"\n",
    "\n",
    "# https://pymupdf.readthedocs.io/en/latest/installation.html#enabling-integrated-ocr-support\n",
    "tessdata = \"path to tessdata in Tesseract OCR .../.../Tesseract-OCR/tessdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the files dataframe\n",
    "df_pdf = pd.read_json(path_to_files_list)\\\n",
    "                    .query('ImagePDF.notna() & ImagePDF==False').reset_index(drop=True)\n",
    "df_pdf['pdf_path'] = df_pdf.apply(lambda x: x['orig_filepath'] if x['filetype'] == 'pdf' \n",
    "                                  else x['converted_filepath'],axis=1)\n",
    "df_pdf = df_pdf[df_pdf.pdf_path.notna()]\\\n",
    "                    .sort_values(by = 'page_count',ascending=True)\\\n",
    "                    .reset_index(drop=True)\n",
    "print(\"normal pdf files to be processed:\",len(df_pdf))\n",
    "\n",
    "# keeping image pdf as separate dataframe\n",
    "df_imagepdf = pd.read_json(path_to_files_list)\\\n",
    "                    .query('ImagePDF.notna() & ImagePDF==True').reset_index(drop=True)\n",
    "df_imagepdf['pdf_path'] = df_imagepdf.apply(lambda x: x['orig_filepath'] if x['filetype'] == 'pdf' \n",
    "                                  else x['converted_filepath'],axis=1)\n",
    "df_imagepdf = df_imagepdf[df_imagepdf.pdf_path.notna()]\\\n",
    "                    .sort_values(by = 'page_count',ascending=True)\\\n",
    "                    .reset_index(drop=True)\n",
    "print(\"image pdf files to be processed:\",len(df_imagepdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdf['path_to_docs'] = df_pdf.progress_apply(lambda x: \n",
    "                    pymuprocessor.create_markdown(filepath=x['pdf_path'],\n",
    "                    folder_location=path_to_save_output_from_pymuprocessor,\n",
    "                    filename=os.path.splitext(os.path.basename(x['pdf_path']))[0]),axis=1)\n",
    "\n",
    "jsonfile = df_pdf.to_json(orient=\"records\")\n",
    "parsed = json.loads(jsonfile)\n",
    "with open(path_to_save_output_from_pymuprocessor + 'pymupdf_markdown_files.json', 'w') as file:\n",
    "    json.dump(parsed, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing imagepdf using ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.45it/s]\n"
     ]
    }
   ],
   "source": [
    "df_imagepdf['path_to_docs'] = df_imagepdf.progress_apply(lambda x: \n",
    "                    pymuprocessor.useOCR_create_text(filepath=x['pdf_path'],\n",
    "                    tessdata=tessdata,\n",
    "                    folder_location=path_to_save_output_from_pymuprocessor,\n",
    "                    filename=os.path.splitext(os.path.basename(x['pdf_path']))[0]),axis=1)\n",
    "\n",
    "jsonfile = df_imagepdf.to_json(orient=\"records\")\n",
    "parsed = json.loads(jsonfile)\n",
    "with open(path_to_save_output_from_pymuprocessor + 'pymupdf_text_files.json', 'w') as file:\n",
    "    json.dump(parsed, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_chunks(filename, path_to_docs, file_extension = 'md'):\n",
    "    # create chunks\n",
    "    chunks = pymuprocessor.create_chunks(folder_location=path_to_docs,overlap= 10,\n",
    "                                         chunk_size=800,file_extension=file_extension)\n",
    "    # save chunks\n",
    "    with open(path_to_save_output_from_pymuprocessor+f'tmp/{filename}/{filename}.chunks.json', 'w') as file:\n",
    "        json.dump(chunks, file)\n",
    "    # return chunks filepath\n",
    "    return path_to_save_output_from_pymuprocessor+f'{filename}/{filename}.chunks.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process normal pdf as processing returned  pagewise markdown files\n",
    "df_pdf['chunks_filepath'] = df_pdf.progress_apply(lambda x: create_save_chunks(\n",
    "                            filename=os.path.splitext(os.path.basename(x['pdf_path']))[0],\n",
    "                            path_to_docs= x['path_to_docs']) if x['path_to_docs'] else None,axis=1)\n",
    "\n",
    "# process image_pdf as processing returned  pagewise text files\n",
    "df_imagepdf['chunks_filepath'] = df_imagepdf.progress_apply(lambda x: create_save_chunks(\n",
    "                            filename=os.path.splitext(os.path.basename(x['pdf_path']))[0],\n",
    "                            path_to_docs= x['path_to_docs'],file_extension='txt') \n",
    "                                    if x['path_to_docs'] else None,axis=1)\n",
    "\n",
    "# save the rpcoess files metadata info\n",
    "df = pd.concat([df_pdf,df_imagepdf],ignore_index=True)\n",
    "import json\n",
    "jsonfile = df.to_json(orient=\"records\")\n",
    "parsed = json.loads(jsonfile)\n",
    "with open(path_to_save_output_from_pymuprocessor + 'processed_chunks.json', 'w') as file:\n",
    "    json.dump(parsed, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagewise_text(folder_location):\n",
    "    pages = utils.get_files(folder_location,file_extensions = \"*\")\n",
    "    pages = pages[\"allfiles\"]\n",
    "    # sort the pages\n",
    "    pages.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "    tmp = []\n",
    "    for page in pages:\n",
    "        with open(page, 'r') as f:\n",
    "            markdown_string = f.read()\n",
    "        tmp.append(markdown_string)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.chunks_filepath.notna()].reset_index(drop=True)\n",
    "df['pages'] = df.progress_apply(lambda x: get_pagewise_text(x['path_to_docs']),axis=1)\n",
    "\n",
    "# check for text quality page wise\n",
    "def check_pages(pages):\n",
    "    page_check= []\n",
    "    for page in pages:\n",
    "        # using gibberish function to detect quality\n",
    "        page_check.append(utils.is_gibberish(page))\n",
    "    return page_check\n",
    "\n",
    "df['gibberish_page_check'] = df.pages.progress_apply(lambda x: check_pages(x))\n",
    "df['extracted_page_count'] = df.gibberish_page_check.apply(lambda x: len(x))\n",
    "df = df[df.extracted_page_count != 0].reset_index(drop=True)\n",
    "\n",
    "df['gibberish_doc_percent'] = df.gibberish_page_check.progress_apply(lambda x: round((sum(x)/len(x)),2))\n",
    "# checking files percent based on percentage of pages good\n",
    "# usually from the experience its seen its a good benchmark\n",
    "# reason for .35 thres is from fact that if document is only 3 pages and of these 1 page is not good \n",
    "# its better to check.\n",
    "print(\"Total Number of okay files:\", len(df[df.gibberish_doc_percent <= 0.35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot to see the distribution of documents which have atleast one page being classified as gibberish\n",
    "df['gibberish_page_count'] = df.gibberish_page_check.apply(lambda x: sum(x))\n",
    "print(\"Total docs having atleast 1 page classified as gibberish:\", \n",
    "                        len(df.query('gibberish_doc_percent > 0')))\n",
    "plt.hist(df.query('gibberish_doc_percent > 0').gibberish_doc_percent, color='lightgreen', \n",
    "                                                                ec='black', bins=20)\n",
    "plt.title(\"Distribution of Percentage of Gibberish page per doc (filtered: >0%)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
